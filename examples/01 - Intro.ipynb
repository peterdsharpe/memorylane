{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcd46b5d",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# MemoryLane: Line-by-Line Memory Profiler\n",
    "\n",
    "MemoryLane is a powerful Python profiler that shows memory usage **after each executed source line**. It helps you identify memory bottlenecks, understand memory allocation patterns, and optimize your code's memory efficiency.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Line-by-line tracking**: See memory usage after every executed line\n",
    "- **Delta reporting**: Track memory changes between lines\n",
    "- **Peak memory tracking**: Monitor high-water memory usage\n",
    "- **Nested function support**: Profile through function calls\n",
    "- **PyTorch integration**: Built-in support for CUDA and CPU memory tracking\n",
    "- **VS Code integration**: Ctrl+click line numbers to jump to source code\n",
    "\n",
    "Let's dive into some examples!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b81b053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from memorylane import profile\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Clear any existing CUDA memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e10580f",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Basic Demo\n",
    "\n",
    "Let's start with a simple example that shows how MemoryLane works. The `@profile` decorator will trace every line execution and show memory usage:\n",
    "\n",
    "### Understanding the Output\n",
    "\n",
    "Each line shows:\n",
    "- **Mem**: Current total allocated memory\n",
    "- **ΔMem**: Change in memory since the previous traced line\n",
    "- **Peak**: Peak memory usage seen so far\n",
    "- **ΔPeak**: Change in peak memory\n",
    "- **Line number**: Clickable line reference (Ctrl+click in VS Code to jump to source)\n",
    "- **Source code**: The actual line that was executed\n",
    "\n",
    "⚠️ **Important**: You only see lines that actually execute, in the order they execute. This means:\n",
    "- Conditional code that doesn't run won't appear\n",
    "- Loop iterations will be \"unrolled\" showing each iteration\n",
    "- Multi-line expressions execute sub-expressions before assignments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50d3a135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \u001b[1;35m━━━━━━ MemoryLane: Line-by-Line Memory Profiler ━━━━━━\u001b[0m\n",
      " \u001b[1mTracing \u001b[0m\u001b[1;36m'basic_demo'\u001b[0m \u001b[1m(\u001b[0mfile: \u001b[90m/tmp/ipykernel_1960718/\u001b[0m\u001b[90m1200114249.py\u001b[0m\u001b[90m:\u001b[0m\u001b[1;90m1\u001b[0m\u001b[1m)\u001b[0m:\n",
      " \u001b[2mMem:      0 MB\u001b[0m | \u001b[2mΔMem:      0 MB\u001b[0m | \u001b[2mPeak:      0 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m1200114249.py:6   \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[97;40mN\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40m \u001b[0m\u001b[37;40m5000\u001b[0m\u001b[97;40m  \u001b[0m\u001b[37;40m# 5000x5000 = 25M elements * 4 bytes = ~100MB\u001b[0m\n",
      " \u001b[1;32mMem:     96 MB\u001b[0m | \u001b[1;32mΔMem:     96 MB\u001b[0m | \u001b[1;32mPeak:     96 MB\u001b[0m | \u001b[1;32mΔPeak:     96 MB\u001b[0m | \u001b[90m1200114249.py:7   \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[97;40mx\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mtorch\u001b[0m\u001b[91;40m.\u001b[0m\u001b[97;40mrandn\u001b[0m\u001b[97;40m(\u001b[0m\u001b[97;40mN\u001b[0m\u001b[97;40m,\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mN\u001b[0m\u001b[97;40m,\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mdevice\u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40mdevice\u001b[0m\u001b[97;40m)\u001b[0m\n",
      " \u001b[1;32mMem:    200 MB\u001b[0m | \u001b[1;32mΔMem:    104 MB\u001b[0m | \u001b[1;32mPeak:    200 MB\u001b[0m | \u001b[1;32mΔPeak:    104 MB\u001b[0m | \u001b[90m1200114249.py:10  \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[97;40my\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mx\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m@\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mx\u001b[0m\u001b[97;40m  \u001b[0m\u001b[37;40m# Peak memory will spike during computation\u001b[0m\n",
      " \u001b[2mMem:    200 MB\u001b[0m | \u001b[2mΔMem:      0 MB\u001b[0m | \u001b[2mPeak:    200 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m1200114249.py:13  \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[97;40my\u001b[0m\u001b[91;40m.\u001b[0m\u001b[97;40mrelu_\u001b[0m\u001b[97;40m(\u001b[0m\u001b[97;40m)\u001b[0m\n",
      " \u001b[2mMem:    200 MB\u001b[0m | \u001b[2mΔMem:      0 MB\u001b[0m | \u001b[2mPeak:    200 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m1200114249.py:16  \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[97;40mresult\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40my\u001b[0m\u001b[91;40m.\u001b[0m\u001b[97;40msum\u001b[0m\u001b[97;40m(\u001b[0m\u001b[97;40m)\u001b[0m\n",
      " \u001b[2mMem:    200 MB\u001b[0m | \u001b[2mΔMem:      0 MB\u001b[0m | \u001b[2mPeak:    200 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m1200114249.py:18  \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[96;40mreturn\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mresult\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(7.0548e+08, device='cuda:0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@profile\n",
    "def basic_demo():\n",
    "    \"\"\"Basic example showing memory allocation patterns.\"\"\"\n",
    "    \n",
    "    # Create a large tensor - this will allocate significant memory\n",
    "    N = 5000  # 5000x5000 = 25M elements * 4 bytes = ~100MB\n",
    "    x = torch.randn(N, N, device=device)\n",
    "    \n",
    "    # Matrix multiplication temporarily uses more memory\n",
    "    y = x @ x  # Peak memory will spike during computation\n",
    "    \n",
    "    # In-place operation should not increase memory\n",
    "    y.relu_()\n",
    "    \n",
    "    # Memory reduction through aggregation\n",
    "    result = y.sum()\n",
    "    \n",
    "    return result\n",
    "\n",
    "basic_demo()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0586da",
   "metadata": {},
   "source": [
    "Important note: note how the `Peak` column is *not* cleared between runs - it reflects the actual cached memory amount. In a sense, the `Peak` is the *process-wide running memory maximum*. \n",
    "\n",
    "This turns out to be quite useful, since it allows you to capture peaks that happen inside inner-scope expressions. But this can also lead to confusion, since it is not limited to the scope of the decorated function. Notice that if we re-run the `basic_demo` function, the peak memory is *not* reset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fd1f04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \u001b[1;35m━━━━━━ MemoryLane: Line-by-Line Memory Profiler ━━━━━━\u001b[0m\n",
      " \u001b[1mTracing \u001b[0m\u001b[1;36m'basic_demo'\u001b[0m \u001b[1m(\u001b[0mfile: \u001b[90m/tmp/ipykernel_1960718/\u001b[0m\u001b[90m1200114249.py\u001b[0m\u001b[90m:\u001b[0m\u001b[1;90m1\u001b[0m\u001b[1m)\u001b[0m:\n",
      " \u001b[2mMem:      8 MB\u001b[0m | \u001b[2mΔMem:      0 MB\u001b[0m | \u001b[2mPeak:    200 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m1200114249.py:6   \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[97;40mN\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40m \u001b[0m\u001b[37;40m5000\u001b[0m\u001b[97;40m  \u001b[0m\u001b[37;40m# 5000x5000 = 25M elements * 4 bytes = ~100MB\u001b[0m\n",
      " \u001b[1;32mMem:    104 MB\u001b[0m | \u001b[1;32mΔMem:     96 MB\u001b[0m | \u001b[2mPeak:    200 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m1200114249.py:7   \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[97;40mx\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mtorch\u001b[0m\u001b[91;40m.\u001b[0m\u001b[97;40mrandn\u001b[0m\u001b[97;40m(\u001b[0m\u001b[97;40mN\u001b[0m\u001b[97;40m,\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mN\u001b[0m\u001b[97;40m,\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mdevice\u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40mdevice\u001b[0m\u001b[97;40m)\u001b[0m\n",
      " \u001b[1;32mMem:    200 MB\u001b[0m | \u001b[1;32mΔMem:     96 MB\u001b[0m | \u001b[2mPeak:    200 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m1200114249.py:10  \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[97;40my\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mx\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m@\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mx\u001b[0m\u001b[97;40m  \u001b[0m\u001b[37;40m# Peak memory will spike during computation\u001b[0m\n",
      " \u001b[2mMem:    200 MB\u001b[0m | \u001b[2mΔMem:      0 MB\u001b[0m | \u001b[2mPeak:    200 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m1200114249.py:13  \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[97;40my\u001b[0m\u001b[91;40m.\u001b[0m\u001b[97;40mrelu_\u001b[0m\u001b[97;40m(\u001b[0m\u001b[97;40m)\u001b[0m\n",
      " \u001b[2mMem:    200 MB\u001b[0m | \u001b[2mΔMem:      0 MB\u001b[0m | \u001b[2mPeak:    200 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m1200114249.py:16  \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[97;40mresult\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40my\u001b[0m\u001b[91;40m.\u001b[0m\u001b[97;40msum\u001b[0m\u001b[97;40m(\u001b[0m\u001b[97;40m)\u001b[0m\n",
      " \u001b[2mMem:    200 MB\u001b[0m | \u001b[2mΔMem:      0 MB\u001b[0m | \u001b[2mPeak:    200 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m1200114249.py:18  \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[96;40mreturn\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mresult\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(7.0527e+08, device='cuda:0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c764226",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Memory Optimization Example\n",
    "\n",
    "Let's look at a function with suboptimal memory usage and then optimize it. This shows how MemoryLane helps identify memory bottlenecks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc0d047e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \u001b[1;35m━━━━━━ MemoryLane: Line-by-Line Memory Profiler ━━━━━━\u001b[0m\n",
      " \u001b[1mTracing \u001b[0m\u001b[1;36m'inefficient_computation'\u001b[0m \u001b[1m(\u001b[0mfile: \u001b[90m/tmp/ipykernel_1960718/\u001b[0m\u001b[90m260837587.py\u001b[0m\u001b[90m:\u001b[0m\u001b[1;90m5\u001b[0m\u001b[1m)\u001b[0m:\n",
      " \u001b[2mMem:      8 MB\u001b[0m | \u001b[2mΔMem:      0 MB\u001b[0m | \u001b[2mPeak:    200 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m260837587.py:10  \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[97;40mtorch\u001b[0m\u001b[91;40m.\u001b[0m\u001b[97;40mmanual_seed\u001b[0m\u001b[97;40m(\u001b[0m\u001b[97;40m_seed\u001b[0m\u001b[97;40m)\u001b[0m\n",
      " \u001b[1;32mMem:    104 MB\u001b[0m | \u001b[1;32mΔMem:     96 MB\u001b[0m | \u001b[2mPeak:    200 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m260837587.py:11  \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[97;40mx\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mtorch\u001b[0m\u001b[91;40m.\u001b[0m\u001b[97;40mrandn\u001b[0m\u001b[97;40m(\u001b[0m\u001b[97;40mN\u001b[0m\u001b[97;40m,\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mN\u001b[0m\u001b[97;40m,\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mdevice\u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40mdevice\u001b[0m\u001b[97;40m)\u001b[0m\n",
      " \u001b[1;32mMem:    200 MB\u001b[0m | \u001b[1;32mΔMem:     96 MB\u001b[0m | \u001b[2mPeak:    200 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m260837587.py:14  \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[97;40mtemp1\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mx\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m*\u001b[0m\u001b[97;40m \u001b[0m\u001b[37;40m2\u001b[0m\n",
      " \u001b[1;32mMem:    296 MB\u001b[0m | \u001b[1;32mΔMem:     96 MB\u001b[0m | \u001b[1;32mPeak:    296 MB\u001b[0m | \u001b[1;32mΔPeak:     96 MB\u001b[0m | \u001b[90m260837587.py:15  \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[97;40mtemp2\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mtemp1\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m+\u001b[0m\u001b[97;40m \u001b[0m\u001b[37;40m1\u001b[0m\n",
      " \u001b[1;32mMem:    392 MB\u001b[0m | \u001b[1;32mΔMem:     96 MB\u001b[0m | \u001b[1;32mPeak:    392 MB\u001b[0m | \u001b[1;32mΔPeak:     96 MB\u001b[0m | \u001b[90m260837587.py:16  \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[97;40mtemp3\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mtemp2\u001b[0m\u001b[91;40m.\u001b[0m\u001b[97;40mpow\u001b[0m\u001b[97;40m(\u001b[0m\u001b[37;40m2\u001b[0m\u001b[97;40m)\u001b[0m\n",
      " \u001b[1;32mMem:    488 MB\u001b[0m | \u001b[1;32mΔMem:     96 MB\u001b[0m | \u001b[1;32mPeak:    488 MB\u001b[0m | \u001b[1;32mΔPeak:     96 MB\u001b[0m | \u001b[90m260837587.py:17  \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[97;40mtemp4\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mtemp3\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m-\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mx\u001b[0m\n",
      " \u001b[1;32mMem:    584 MB\u001b[0m | \u001b[1;32mΔMem:     96 MB\u001b[0m | \u001b[1;32mPeak:    584 MB\u001b[0m | \u001b[1;32mΔPeak:     96 MB\u001b[0m | \u001b[90m260837587.py:18  \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[97;40mtemp5\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mtemp4\u001b[0m\u001b[91;40m.\u001b[0m\u001b[97;40mrelu\u001b[0m\u001b[97;40m(\u001b[0m\u001b[97;40m)\u001b[0m\n",
      " \u001b[2mMem:    584 MB\u001b[0m | \u001b[2mΔMem:      0 MB\u001b[0m | \u001b[2mPeak:    584 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m260837587.py:19  \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[97;40mresult\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mtemp5\u001b[0m\u001b[91;40m.\u001b[0m\u001b[97;40mmean\u001b[0m\u001b[97;40m(\u001b[0m\u001b[97;40m)\u001b[0m\n",
      " \u001b[2mMem:    584 MB\u001b[0m | \u001b[2mΔMem:      0 MB\u001b[0m | \u001b[2mPeak:    584 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m260837587.py:21  \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[96;40mreturn\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mresult\u001b[0m\n",
      " \u001b[1;35m━━━━━━ MemoryLane: Line-by-Line Memory Profiler ━━━━━━\u001b[0m\n",
      " \u001b[1mTracing \u001b[0m\u001b[1;36m'efficient_computation'\u001b[0m \u001b[1m(\u001b[0mfile: \u001b[90m/tmp/ipykernel_1960718/\u001b[0m\u001b[90m260837587.py\u001b[0m\u001b[90m:\u001b[0m\u001b[1;90m23\u001b[0m\u001b[1m)\u001b[0m:\n",
      " \u001b[2mMem:      8 MB\u001b[0m | \u001b[2mΔMem:      0 MB\u001b[0m | \u001b[2mPeak:    584 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m260837587.py:28  \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[97;40mtorch\u001b[0m\u001b[91;40m.\u001b[0m\u001b[97;40mmanual_seed\u001b[0m\u001b[97;40m(\u001b[0m\u001b[97;40m_seed\u001b[0m\u001b[97;40m)\u001b[0m\n",
      " \u001b[1;32mMem:    104 MB\u001b[0m | \u001b[1;32mΔMem:     96 MB\u001b[0m | \u001b[2mPeak:    584 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m260837587.py:29  \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[97;40mx\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mtorch\u001b[0m\u001b[91;40m.\u001b[0m\u001b[97;40mrandn\u001b[0m\u001b[97;40m(\u001b[0m\u001b[97;40mN\u001b[0m\u001b[97;40m,\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mN\u001b[0m\u001b[97;40m,\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mdevice\u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40mdevice\u001b[0m\u001b[97;40m)\u001b[0m\n",
      " \u001b[2mMem:    104 MB\u001b[0m | \u001b[2mΔMem:      0 MB\u001b[0m | \u001b[2mPeak:    584 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m260837587.py:33  \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[97;40mresult\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40m(\u001b[0m\u001b[97;40m(\u001b[0m\u001b[97;40mx\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m*\u001b[0m\u001b[97;40m \u001b[0m\u001b[37;40m2\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m+\u001b[0m\u001b[97;40m \u001b[0m\u001b[37;40m1\u001b[0m\u001b[97;40m)\u001b[0m\u001b[91;40m.\u001b[0m\u001b[97;40mpow_\u001b[0m\u001b[97;40m(\u001b[0m\u001b[37;40m2\u001b[0m\u001b[97;40m)\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m-\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mx\u001b[0m\u001b[97;40m)\u001b[0m\u001b[91;40m.\u001b[0m\u001b[97;40mrelu_\u001b[0m\u001b[97;40m(\u001b[0m\u001b[97;40m)\u001b[0m\u001b[91;40m.\u001b[0m\u001b[97;40mmean\u001b[0m\u001b[97;40m(\u001b[0m\u001b[97;40m)\u001b[0m\n",
      " \u001b[2mMem:    104 MB\u001b[0m | \u001b[2mΔMem:      0 MB\u001b[0m | \u001b[2mPeak:    584 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m260837587.py:35  \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[96;40mreturn\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mresult\u001b[0m\n",
      "\n",
      "Comparison:\n",
      "Both functions computed same results: True\n",
      "Notice how the optimized version uses ~1/6 the memory (104 MB vs. 584 MB)!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### [Seeded Randomness for Reproducibility]\n",
    "N = 5000\n",
    "_seed: int = 42  # Chosen seed for reproducibility\n",
    "\n",
    "@profile\n",
    "def inefficient_computation():\n",
    "    \"\"\"Example with poor memory efficiency - creating unnecessary intermediate tensors.\n",
    "    \"\"\"\n",
    "    # Set the random seed for deterministic results\n",
    "    torch.manual_seed(_seed)\n",
    "    x = torch.randn(N, N, device=device)\n",
    "\n",
    "    # BAD: Creating many intermediate tensors that all stay in memory\n",
    "    temp1 = x * 2\n",
    "    temp2 = temp1 + 1\n",
    "    temp3 = temp2.pow(2)\n",
    "    temp4 = temp3 - x\n",
    "    temp5 = temp4.relu()\n",
    "    result = temp5.mean()\n",
    "\n",
    "    return result\n",
    "\n",
    "@profile\n",
    "def efficient_computation():\n",
    "    \"\"\"Optimized version using in-place operations and chaining.\n",
    "    \"\"\"\n",
    "    # Set the random seed for deterministic results\n",
    "    torch.manual_seed(_seed)\n",
    "    x = torch.randn(N, N, device=device)\n",
    "\n",
    "    # BETTER: Chain operations and use in-place when possible\n",
    "    # This reduces intermediate tensor allocations\n",
    "    result = ((x * 2 + 1).pow_(2) - x).relu_().mean()\n",
    "\n",
    "    return result\n",
    "\n",
    "result1 = inefficient_computation()\n",
    "result2 = efficient_computation()\n",
    "\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"Both functions computed same results: {torch.all(result1 == result2)}\")\n",
    "print(f\"Notice how the optimized version uses ~1/6 the memory (104 MB vs. 584 MB)!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b218e1",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Nested Function Tracing\n",
    "\n",
    "MemoryLane can trace through nested function calls. When a decorated function calls another decorated function, both are traced with proper indentation to show the call hierarchy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f3d684d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NESTED FUNCTION TRACING ===\n",
      " \u001b[1;35m━━━━━━ MemoryLane: Line-by-Line Memory Profiler ━━━━━━\u001b[0m\n",
      " \u001b[1mTracing \u001b[0m\u001b[1;36m'main_function'\u001b[0m \u001b[1m(\u001b[0mfile: \u001b[90m/tmp/ipykernel_1960718/\u001b[0m\u001b[90m1308085490.py\u001b[0m\u001b[90m:\u001b[0m\u001b[1;90m7\u001b[0m\u001b[1m)\u001b[0m:\n",
      " \u001b[1;32mMem:     24 MB\u001b[0m | \u001b[1;32mΔMem:     16 MB\u001b[0m | \u001b[2mPeak:    584 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m1308085490.py:11  \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[97;40mdata\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mtorch\u001b[0m\u001b[91;40m.\u001b[0m\u001b[97;40mrandn\u001b[0m\u001b[97;40m(\u001b[0m\u001b[37;40m2000\u001b[0m\u001b[97;40m,\u001b[0m\u001b[97;40m \u001b[0m\u001b[37;40m2000\u001b[0m\u001b[97;40m,\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mdevice\u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40mdevice\u001b[0m\u001b[97;40m)\u001b[0m\n",
      " \u001b[2mMem:     24 MB\u001b[0m | \u001b[2mΔMem:      0 MB\u001b[0m | \u001b[2mPeak:    584 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m1308085490.py:14  \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[97;40mchunk1\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mdata\u001b[0m\u001b[97;40m[\u001b[0m\u001b[97;40m:\u001b[0m\u001b[37;40m1000\u001b[0m\u001b[97;40m]\u001b[0m\n",
      " \u001b[2mMem:     24 MB\u001b[0m | \u001b[2mΔMem:      0 MB\u001b[0m | \u001b[2mPeak:    584 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m1308085490.py:15  \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[97;40mchunk2\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mdata\u001b[0m\u001b[97;40m[\u001b[0m\u001b[37;40m1000\u001b[0m\u001b[97;40m:\u001b[0m\u001b[97;40m]\u001b[0m\n",
      "     \u001b[1mTracing \u001b[0m\u001b[1;36m'child_function'\u001b[0m \u001b[1m(\u001b[0mfile: \u001b[90m/tmp/ipykernel_1960718/\u001b[0m\u001b[90m1308085490.py\u001b[0m\u001b[90m:\u001b[0m\u001b[1;90m1\u001b[0m\u001b[1m)\u001b[0m:\n",
      "     \u001b[1;32mMem:     32 MB\u001b[0m | \u001b[1;32mΔMem:      8 MB\u001b[0m | \u001b[2mPeak:    584 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m1308085490.py:3   \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[97;40my\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mtorch\u001b[0m\u001b[91;40m.\u001b[0m\u001b[97;40mzeros_like\u001b[0m\u001b[97;40m(\u001b[0m\u001b[97;40mx\u001b[0m\u001b[97;40m)\u001b[0m\n",
      "     \u001b[1;32mMem:     39 MB\u001b[0m | \u001b[1;32mΔMem:      8 MB\u001b[0m | \u001b[2mPeak:    584 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m1308085490.py:4   \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[97;40mresult\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mx\u001b[0m\u001b[91;40m.\u001b[0m\u001b[97;40mpow\u001b[0m\u001b[97;40m(\u001b[0m\u001b[37;40m2\u001b[0m\u001b[97;40m)\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m+\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40my\u001b[0m\n",
      "     \u001b[2mMem:     39 MB\u001b[0m | \u001b[2mΔMem:      0 MB\u001b[0m | \u001b[2mPeak:    584 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m1308085490.py:5   \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[96;40mreturn\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mresult\u001b[0m\u001b[91;40m.\u001b[0m\u001b[97;40msum\u001b[0m\u001b[97;40m(\u001b[0m\u001b[97;40mdim\u001b[0m\u001b[91;40m=\u001b[0m\u001b[91;40m-\u001b[0m\u001b[37;40m1\u001b[0m\u001b[97;40m)\u001b[0m\n",
      " \u001b[2mMem:     24 MB\u001b[0m | \u001b[2mΔMem:      0 MB\u001b[0m | \u001b[2mPeak:    584 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m1308085490.py:18  \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[97;40mresult1\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mchild_function\u001b[0m\u001b[97;40m(\u001b[0m\u001b[97;40mchunk1\u001b[0m\u001b[97;40m)\u001b[0m\n",
      "     \u001b[1mTracing \u001b[0m\u001b[1;36m'child_function'\u001b[0m \u001b[1m(\u001b[0mfile: \u001b[90m/tmp/ipykernel_1960718/\u001b[0m\u001b[90m1308085490.py\u001b[0m\u001b[90m:\u001b[0m\u001b[1;90m1\u001b[0m\u001b[1m)\u001b[0m:\n",
      "     \u001b[1;32mMem:     32 MB\u001b[0m | \u001b[1;32mΔMem:      8 MB\u001b[0m | \u001b[2mPeak:    584 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m1308085490.py:3   \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[97;40my\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mtorch\u001b[0m\u001b[91;40m.\u001b[0m\u001b[97;40mzeros_like\u001b[0m\u001b[97;40m(\u001b[0m\u001b[97;40mx\u001b[0m\u001b[97;40m)\u001b[0m\n",
      "     \u001b[1;32mMem:     39 MB\u001b[0m | \u001b[1;32mΔMem:      8 MB\u001b[0m | \u001b[2mPeak:    584 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m1308085490.py:4   \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[97;40mresult\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mx\u001b[0m\u001b[91;40m.\u001b[0m\u001b[97;40mpow\u001b[0m\u001b[97;40m(\u001b[0m\u001b[37;40m2\u001b[0m\u001b[97;40m)\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m+\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40my\u001b[0m\n",
      "     \u001b[2mMem:     39 MB\u001b[0m | \u001b[2mΔMem:      0 MB\u001b[0m | \u001b[2mPeak:    584 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m1308085490.py:5   \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[96;40mreturn\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mresult\u001b[0m\u001b[91;40m.\u001b[0m\u001b[97;40msum\u001b[0m\u001b[97;40m(\u001b[0m\u001b[97;40mdim\u001b[0m\u001b[91;40m=\u001b[0m\u001b[91;40m-\u001b[0m\u001b[37;40m1\u001b[0m\u001b[97;40m)\u001b[0m\n",
      " \u001b[2mMem:     24 MB\u001b[0m | \u001b[2mΔMem:      0 MB\u001b[0m | \u001b[2mPeak:    584 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m1308085490.py:19  \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[97;40mresult2\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mchild_function\u001b[0m\u001b[97;40m(\u001b[0m\u001b[97;40mchunk2\u001b[0m\u001b[97;40m)\u001b[0m\n",
      " \u001b[2mMem:     24 MB\u001b[0m | \u001b[2mΔMem:      0 MB\u001b[0m | \u001b[2mPeak:    584 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m1308085490.py:22  \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[97;40mfinal_result\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mresult1\u001b[0m\u001b[91;40m.\u001b[0m\u001b[97;40mmean\u001b[0m\u001b[97;40m(\u001b[0m\u001b[97;40m)\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m+\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mresult2\u001b[0m\u001b[91;40m.\u001b[0m\u001b[97;40mmean\u001b[0m\u001b[97;40m(\u001b[0m\u001b[97;40m)\u001b[0m\n",
      " \u001b[2mMem:     24 MB\u001b[0m | \u001b[2mΔMem:      0 MB\u001b[0m | \u001b[2mPeak:    584 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m1308085490.py:24  \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[96;40mreturn\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mfinal_result\u001b[0m\n",
      "\n",
      "Final output: 4001.4329\n",
      "Notice how child function calls are indented to show the call hierarchy!\n"
     ]
    }
   ],
   "source": [
    "@profile\n",
    "def child_function(x):\n",
    "    y = torch.zeros_like(x)\n",
    "    result = x.pow(2) + y\n",
    "    return result.sum(dim=-1)\n",
    "\n",
    "@profile  \n",
    "def main_function():\n",
    "    \n",
    "    # Initial allocation\n",
    "    data = torch.randn(2000, 2000, device=device)\n",
    "    \n",
    "    # Process in chunks to demonstrate nested calls\n",
    "    chunk1 = data[:1000]\n",
    "    chunk2 = data[1000:]\n",
    "    \n",
    "    # These calls will be traced with indentation\n",
    "    result1 = child_function(chunk1)\n",
    "    result2 = child_function(chunk2)\n",
    "    \n",
    "    # Combine results\n",
    "    final_result = result1.mean() + result2.mean()\n",
    "    \n",
    "    return final_result\n",
    "\n",
    "print(\"=== NESTED FUNCTION TRACING ===\")\n",
    "output = main_function()\n",
    "print(f\"\\nFinal output: {output.item():.4f}\")\n",
    "print(\"Notice how child function calls are indented to show the call hierarchy!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15809b18",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## PyTorch nn.Module Integration\n",
    "\n",
    "MemoryLane works seamlessly with PyTorch modules. You can decorate the `forward` method to trace memory usage during neural network computations. This is particularly useful for understanding memory bottlenecks in deep learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d31e7885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NEURAL NETWORK FORWARD PASS ===\n",
      " \u001b[1;35m━━━━━━ MemoryLane: Line-by-Line Memory Profiler ━━━━━━\u001b[0m\n",
      " \u001b[1mTracing \u001b[0m\u001b[1;36m'forward'\u001b[0m \u001b[1m(\u001b[0mfile: \u001b[90m/tmp/ipykernel_1960718/\u001b[0m\u001b[90m731318301.py\u001b[0m\u001b[90m:\u001b[0m\u001b[1;90m15\u001b[0m\u001b[1m)\u001b[0m:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \u001b[1;32mMem:     39 MB\u001b[0m | \u001b[1;32mΔMem:      2 MB\u001b[0m | \u001b[2mPeak:    584 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m731318301.py:20  \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[97;40mh1\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mself\u001b[0m\u001b[91;40m.\u001b[0m\u001b[97;40mlinear1\u001b[0m\u001b[97;40m(\u001b[0m\u001b[97;40mx\u001b[0m\u001b[97;40m)\u001b[0m\n",
      " \u001b[1;32mMem:     41 MB\u001b[0m | \u001b[1;32mΔMem:      2 MB\u001b[0m | \u001b[2mPeak:    584 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m731318301.py:21  \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[97;40mh1_activated\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mtorch\u001b[0m\u001b[91;40m.\u001b[0m\u001b[97;40mrelu\u001b[0m\u001b[97;40m(\u001b[0m\u001b[97;40mh1\u001b[0m\u001b[97;40m)\u001b[0m\n",
      " \u001b[1;32mMem:     43 MB\u001b[0m | \u001b[1;32mΔMem:      2 MB\u001b[0m | \u001b[2mPeak:    584 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m731318301.py:24  \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[97;40mh2\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mself\u001b[0m\u001b[91;40m.\u001b[0m\u001b[97;40mlinear2\u001b[0m\u001b[97;40m(\u001b[0m\u001b[97;40mh1_activated\u001b[0m\u001b[97;40m)\u001b[0m\n",
      " \u001b[1;32mMem:     45 MB\u001b[0m | \u001b[1;32mΔMem:      2 MB\u001b[0m | \u001b[2mPeak:    584 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m731318301.py:25  \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[97;40mh2_activated\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mtorch\u001b[0m\u001b[91;40m.\u001b[0m\u001b[97;40mrelu\u001b[0m\u001b[97;40m(\u001b[0m\u001b[97;40mh2\u001b[0m\u001b[97;40m)\u001b[0m\n",
      " \u001b[1;32mMem:     47 MB\u001b[0m | \u001b[1;32mΔMem:      2 MB\u001b[0m | \u001b[2mPeak:    584 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m731318301.py:28  \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[97;40mh2_residual\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mh2_activated\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m+\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mh1_activated\u001b[0m\n",
      " \u001b[1;32mMem:     49 MB\u001b[0m | \u001b[1;32mΔMem:      2 MB\u001b[0m | \u001b[2mPeak:    584 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m731318301.py:31  \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[97;40mh2_dropped\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mself\u001b[0m\u001b[91;40m.\u001b[0m\u001b[97;40mdropout\u001b[0m\u001b[97;40m(\u001b[0m\u001b[97;40mh2_residual\u001b[0m\u001b[97;40m)\u001b[0m\n",
      " \u001b[1;32mMem:     49 MB\u001b[0m | \u001b[1;32mΔMem:      0 MB\u001b[0m | \u001b[2mPeak:    584 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m731318301.py:34  \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[97;40moutput\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mself\u001b[0m\u001b[91;40m.\u001b[0m\u001b[97;40mlinear3\u001b[0m\u001b[97;40m(\u001b[0m\u001b[97;40mh2_dropped\u001b[0m\u001b[97;40m)\u001b[0m\n",
      " \u001b[2mMem:     49 MB\u001b[0m | \u001b[2mΔMem:      0 MB\u001b[0m | \u001b[2mPeak:    584 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m731318301.py:36  \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[96;40mreturn\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40moutput\u001b[0m\n",
      "\n",
      "Output shape: torch.Size([256, 512])\n",
      "Output mean: 0.0026\n",
      "Output std: 0.3002\n",
      "\n",
      "Tip: You can profile backward passes too by decorating forward() without torch.no_grad()!\n"
     ]
    }
   ],
   "source": [
    "# Clear memory before neural network example\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "class SimpleNetwork(nn.Module):\n",
    "    \"\"\"A simple neural network to demonstrate memory profiling.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=1000, hidden_size=2048, output_size=512):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    @profile  # Decorate the forward method\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass with memory profiling.\"\"\"\n",
    "        \n",
    "        # Layer 1: input -> hidden\n",
    "        h1 = self.linear1(x)\n",
    "        h1_activated = torch.relu(h1)\n",
    "        \n",
    "        # Layer 2: hidden -> hidden (with residual connection)\n",
    "        h2 = self.linear2(h1_activated)\n",
    "        h2_activated = torch.relu(h2)\n",
    "        \n",
    "        # Add residual connection (this creates temporary tensors)\n",
    "        h2_residual = h2_activated + h1_activated\n",
    "        \n",
    "        # Apply dropout\n",
    "        h2_dropped = self.dropout(h2_residual)\n",
    "        \n",
    "        # Layer 3: hidden -> output\n",
    "        output = self.linear3(h2_dropped)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Create and move model to device\n",
    "model = SimpleNetwork().to(device)\n",
    "\n",
    "# Create input data - larger batch to see memory effects\n",
    "batch_size = 256\n",
    "input_data = torch.randn(batch_size, 1000, device=device)\n",
    "\n",
    "print(\"=== NEURAL NETWORK FORWARD PASS ===\")\n",
    "with torch.no_grad():  # Disable gradients for inference\n",
    "    predictions = model(input_data)\n",
    "\n",
    "print(f\"\\nOutput shape: {predictions.shape}\")\n",
    "print(f\"Output mean: {predictions.mean().item():.4f}\")\n",
    "print(f\"Output std: {predictions.std().item():.4f}\")\n",
    "print(\"\\nTip: You can profile backward passes too by decorating forward() without torch.no_grad()!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654ed555",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Advanced Features and Tips\n",
    "\n",
    "### Profiler Configuration\n",
    "\n",
    "MemoryLane supports several configuration options for different use cases:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edeb2a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FILTERED PROFILING (only significant changes) ===\n",
      " \u001b[1;35m━━━━━━ MemoryLane: Line-by-Line Memory Profiler ━━━━━━\u001b[0m\n",
      " \u001b[1mTracing \u001b[0m\u001b[1;36m'filtered_example'\u001b[0m \u001b[1m(\u001b[0mfile: \u001b[90m/tmp/ipykernel_1960718/\u001b[0m\u001b[90m4048755409.py\u001b[0m\u001b[90m:\u001b[0m\u001b[1;90m15\u001b[0m\u001b[1m)\u001b[0m:\n",
      " \u001b[1;32mMem:    133 MB\u001b[0m | \u001b[1;32mΔMem:     96 MB\u001b[0m | \u001b[2mPeak:    584 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m4048755409.py:18  \u001b[0m | \u001b[97;40m    \u001b[0m\u001b[97;40mlarge_tensor\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mtorch\u001b[0m\u001b[91;40m.\u001b[0m\u001b[97;40mrandn\u001b[0m\u001b[97;40m(\u001b[0m\u001b[37;40m5000\u001b[0m\u001b[97;40m,\u001b[0m\u001b[97;40m \u001b[0m\u001b[37;40m5000\u001b[0m\u001b[97;40m,\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mdevice\u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40mdevice\u001b[0m\u001b[97;40m)\u001b[0m\u001b[97;40m  \u001b[0m\u001b[37;40m# Will show\u001b[0m\n",
      "\n",
      "Result: -9510.14\n",
      "Notice: Only the large tensor allocation was shown because we set only_show_significant=True\n"
     ]
    }
   ],
   "source": [
    "# Different memory tracking modes\n",
    "@profile(memory_type=\"torch_cuda\")  # Track CUDA memory (default)\n",
    "def cuda_example():\n",
    "    return torch.randn(100, 100, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "@profile(memory_type=\"torch_cpu\")   # Track CPU memory\n",
    "def cpu_example():\n",
    "    return torch.randn(100, 100, device=\"cpu\")\n",
    "\n",
    "@profile(memory_type=\"python\")      # Track Python memory\n",
    "def python_example():\n",
    "    return [i**2 for i in range(1000)]\n",
    "\n",
    "# Threshold filtering - only show significant changes\n",
    "@profile(threshold=10*1024**2, only_show_significant=True)  # Only show changes > 10MB\n",
    "def filtered_example():\n",
    "    small_tensor = torch.randn(10, 10, device=device)  # Won't show (too small)\n",
    "    large_tensor = torch.randn(5000, 5000, device=device)  # Will show\n",
    "    return large_tensor.sum()\n",
    "\n",
    "print(\"=== FILTERED PROFILING (only significant changes) ===\")\n",
    "filtered_result = filtered_example()\n",
    "\n",
    "print(f\"\\nResult: {filtered_result.item():.2f}\")\n",
    "print(\"Notice: Only the large tensor allocation was shown because we set only_show_significant=True\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab8c183",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Best Practices\n",
    "\n",
    "1. **Start with broad profiling**: Use `@profile` without arguments to get the full picture\n",
    "2. **Use thresholds for noisy code**: Set `threshold` and `only_show_significant=True` for cleaner output\n",
    "3. **Profile incrementally**: Start with main functions, then dive into bottlenecks\n",
    "4. **Check peak memory**: Watch for spikes in the Peak column - these indicate temporary high memory usage\n",
    "5. **Use VS Code integration**: Ctrl+click line numbers to jump directly to problematic code\n",
    "6. **Clear memory between runs**: Use `torch.cuda.empty_cache()` for consistent baselines\n",
    "\n",
    "### What to Look For\n",
    "\n",
    "- **Large ΔMem values**: Indicate significant memory allocations\n",
    "- **Peak spikes**: Show temporary memory usage that might cause OOM errors\n",
    "- **Memory not decreasing**: May indicate memory leaks or inefficient cleanup\n",
    "- **Unexpected execution order**: Remember that loops unroll and expressions evaluate in pieces\n",
    "- **Gradual memory accumulation**: Could indicate memory leaks in loops\n",
    "\n",
    "### Common Debugging Scenarios\n",
    "\n",
    "MemoryLane excels at debugging:\n",
    "- **Out-of-memory errors**: Find which line pushes you over the limit\n",
    "- **Memory leaks**: Identify where memory doesn't get freed as expected\n",
    "- **Inefficient algorithms**: Compare different implementations side-by-side\n",
    "- **Model optimization**: Profile different model architectures or batch sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7883b650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DEBUGGING MEMORY ACCUMULATION ===\n",
      " \u001b[1;35m━━━━━━ MemoryLane: Line-by-Line Memory Profiler ━━━━━━\u001b[0m\n",
      " \u001b[1mTracing \u001b[0m\u001b[1;36m'debug_memory_accumulation'\u001b[0m \u001b[1m(\u001b[0mfile: \u001b[90m/tmp/ipykernel_1960718/\u001b[0m\u001b[90m758106828.py\u001b[0m\u001b[90m:\u001b[0m\u001b[1;90m1\u001b[0m\u001b[1m)\u001b[0m:\n",
      " \u001b[1;32mMem:     45 MB\u001b[0m | \u001b[1;32mΔMem:      8 MB\u001b[0m | \u001b[2mPeak:    584 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m758106828.py:9   \u001b[0m | \u001b[97;40m        \u001b[0m\u001b[97;40mbatch_data\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mtorch\u001b[0m\u001b[91;40m.\u001b[0m\u001b[97;40mrandn\u001b[0m\u001b[97;40m(\u001b[0m\u001b[37;40m1000\u001b[0m\u001b[97;40m,\u001b[0m\u001b[97;40m \u001b[0m\u001b[37;40m2000\u001b[0m\u001b[97;40m,\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mdevice\u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40mdevice\u001b[0m\u001b[97;40m)\u001b[0m\n",
      " \u001b[1;31mMem:     38 MB\u001b[0m | \u001b[1;31mΔMem:     -8 MB\u001b[0m | \u001b[2mPeak:    584 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m758106828.py:16  \u001b[0m | \u001b[97;40m        \u001b[0m\u001b[96;40mdel\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mbatch_data\u001b[0m\n",
      " \u001b[1;32mMem:     45 MB\u001b[0m | \u001b[1;32mΔMem:      8 MB\u001b[0m | \u001b[2mPeak:    584 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m758106828.py:9   \u001b[0m | \u001b[97;40m        \u001b[0m\u001b[97;40mbatch_data\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mtorch\u001b[0m\u001b[91;40m.\u001b[0m\u001b[97;40mrandn\u001b[0m\u001b[97;40m(\u001b[0m\u001b[37;40m1000\u001b[0m\u001b[97;40m,\u001b[0m\u001b[97;40m \u001b[0m\u001b[37;40m2000\u001b[0m\u001b[97;40m,\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mdevice\u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40mdevice\u001b[0m\u001b[97;40m)\u001b[0m\n",
      " \u001b[1;31mMem:     38 MB\u001b[0m | \u001b[1;31mΔMem:     -8 MB\u001b[0m | \u001b[2mPeak:    584 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m758106828.py:16  \u001b[0m | \u001b[97;40m        \u001b[0m\u001b[96;40mdel\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mbatch_data\u001b[0m\n",
      " \u001b[1;32mMem:     45 MB\u001b[0m | \u001b[1;32mΔMem:      8 MB\u001b[0m | \u001b[2mPeak:    584 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m758106828.py:9   \u001b[0m | \u001b[97;40m        \u001b[0m\u001b[97;40mbatch_data\u001b[0m\u001b[97;40m \u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mtorch\u001b[0m\u001b[91;40m.\u001b[0m\u001b[97;40mrandn\u001b[0m\u001b[97;40m(\u001b[0m\u001b[37;40m1000\u001b[0m\u001b[97;40m,\u001b[0m\u001b[97;40m \u001b[0m\u001b[37;40m2000\u001b[0m\u001b[97;40m,\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mdevice\u001b[0m\u001b[91;40m=\u001b[0m\u001b[97;40mdevice\u001b[0m\u001b[97;40m)\u001b[0m\n",
      " \u001b[1;31mMem:     38 MB\u001b[0m | \u001b[1;31mΔMem:     -8 MB\u001b[0m | \u001b[2mPeak:    584 MB\u001b[0m | \u001b[2mΔPeak:      0 MB\u001b[0m | \u001b[90m758106828.py:16  \u001b[0m | \u001b[97;40m        \u001b[0m\u001b[96;40mdel\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mbatch_data\u001b[0m\n",
      "\n",
      "Final result: 2997.80\n",
      "\n",
      "Try uncommenting the 'del batch_data' line to see the difference!\n",
      "This shows how MemoryLane helps identify memory accumulation patterns.\n"
     ]
    }
   ],
   "source": [
    "@profile(threshold=5*1024**2, only_show_significant=True)  # Show changes > 5MB\n",
    "def debug_memory_accumulation():\n",
    "    \"\"\"Example showing how to debug memory accumulation in loops.\"\"\"\n",
    "    \n",
    "    data_list = []\n",
    "    \n",
    "    for i in range(3):  # Small loop for demo\n",
    "        # Each iteration creates a large tensor\n",
    "        batch_data = torch.randn(1000, 2000, device=device)\n",
    "        \n",
    "        # Process the data\n",
    "        processed = batch_data.pow(2).mean(dim=1)\n",
    "        \n",
    "        # Store results - this might accumulate memory!\n",
    "        data_list.append(processed)\n",
    "        del batch_data\n",
    "        # Optional: del batch_data  # Explicitly free intermediate data\n",
    "    \n",
    "    # Combine all results\n",
    "    final_result = torch.stack(data_list).sum()\n",
    "    \n",
    "    return final_result\n",
    "\n",
    "print(\"=== DEBUGGING MEMORY ACCUMULATION ===\")\n",
    "result = debug_memory_accumulation()\n",
    "print(f\"\\nFinal result: {result.item():.2f}\")\n",
    "print(\"\\nTry uncommenting the 'del batch_data' line to see the difference!\")\n",
    "print(\"This shows how MemoryLane helps identify memory accumulation patterns.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
